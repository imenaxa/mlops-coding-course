{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>TODO</p>"},{"location":"0.%20Overview/0.0%20Summary/","title":"The MLOPS template course","text":""},{"location":"0.%20Overview/0.0%20Summary/#in-few-words","title":"In few words","text":""},{"location":"0.%20Overview/0.0%20Summary/#intended-audience","title":"Intended Audience","text":""},{"location":"0.%20Overview/0.0%20Summary/#how-to-read","title":"How to read ?","text":""},{"location":"0.%20Overview/0.0%20Summary/#technology","title":"Technology","text":"<p>Copyright </p>"},{"location":"0.%20Overview/0.1.%20Datasets/","title":"0.1. Data","text":"<p>Data is often referred to as the fuel for Machine Learning, and although this course focuses on MLOps, it's crucial to have access to data to fully grasp the various concepts and technologies involved.</p>"},{"location":"0.%20Overview/0.1.%20Datasets/#types-of-data","title":"Types of Data","text":"<p>When mentionning data, the first point is perhaps what are we talking about. When exploiting the model, data will be required at every step and will take many forms, be stored on different supports and will have different properties.</p> <p>Briefly we can note the following data types:</p>"},{"location":"0.%20Overview/0.1.%20Datasets/#structured-data","title":"Structured Data","text":"<p>Structured data adheres to a predefined model, making it easier to search and organize.</p> <ul> <li>Tabular:      Perhaps the most common type of data, where data is organize in rows and columns.<ul> <li>Column are homogeneous in terms of types</li> <li>Typically CSV files and Relational database </li> </ul> </li> <li>Time Series:     Sequence of data points collected or recorded at successive points in time, usually at consistent intervals. <ul> <li>Characterized by temporal order, meaning the sequence of observations is crucial, and changing the order can alter the meaning or interpretation of the data. </li> <li>Typically, financial data and energy</li> </ul> </li> <li>Geospatial:     Data representing a specific location or geographic area on earth<ul> <li>Can be represented as latitude, longitude or other type of geographic indicators</li> <li>Useful to anlyse and visual spatial relationship and patterns</li> <li>Typically handled by Geographic Information System (GIS)</li> </ul> </li> <li>Graph     Data are organised around vertices and edges that connects them representing entities and relationships between them.<ul> <li>useful to model complex networks and many real workd syste,s</li> <li>graph can be directed, undirected , weighted, multiple, cyclic, acyclic</li> </ul> </li> </ul>"},{"location":"0.%20Overview/0.1.%20Datasets/#unstructured-data","title":"Unstructured Data","text":"<p>Unstructured data does not follow a predefined model, making it more complex to process.</p> <ul> <li>Text:     The most common form of unstructured data corresponding to written content.<ul> <li>can be simple strings to entire book</li> <li>characterized by high number of unique words, contextual meaning and ambiguity</li> </ul> </li> <li>Multimedia:     Refer to picture, sound, video data<ul> <li>challenging due to the high dimensionality, large file sizes, and the complexity of extracting meaningful patterns. </li> </ul> </li> </ul>"},{"location":"0.%20Overview/0.1.%20Datasets/#semi-structured-data","title":"Semi Structured Data","text":"<p>Data that does not conform to a rigid data model like structured data, but it does contain tags or other markers to separate semantic elements and enforce hierarchies of records and fields, making it easier to parse than unstructured data. Examples are XML and JSON files.</p>"},{"location":"0.%20Overview/0.1.%20Datasets/#which-data-should-i-use","title":"Which data should I use?","text":"<p>The question of which dataset to use is common, and honestly, the best dataset is the one you're most familiar with.  While the vast array of data types and their diverse applications might seem overwhelming, it's important to remember that many MLOps concepts are universal and can be applied across different domains.</p> <p>We will look into the specificities of certain types of applications later in the course. For now, we offer two options for getting started.</p>"},{"location":"0.%20Overview/0.1.%20Datasets/#option-1-use-your-own-dataset-recommended","title":"Option 1: Use your own dataset (recommended)","text":"<p>Whenever possible, applying what you learn to your own dataset is highly recommended. This approach has several advantages:</p> <ul> <li>Relevance: Working with your data means the insights and models you develop are immediately applicable to your projects or interests.</li> <li>Familiarity: You are likely more familiar with the specificties of your own data, which will ease the analysis and troubleshooting.</li> <li>Customization: It allows you to tailor the MLOps processes and solutions directly to your context.</li> </ul>"},{"location":"0.%20Overview/0.1.%20Datasets/#option-2-use-a-dataset-from-kaggle","title":"Option 2: Use a dataset from Kaggle","text":"<p>If you don't have your own dataset or are looking for a new challenge, we recommend starting with a well-documented and widely-used public dataset. For this course, we suggest the following dataset from Kaggle:</p> <ul> <li>Name: House Prices - Advanced Regression Techniques</li> <li>Source: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview</li> </ul>"},{"location":"0.%20Overview/0.2.%20Architecture/","title":"Why not specific tools?","text":"<p>Databricks, metaflow ...</p> <p>=&gt; We want something neutral that can be used on every platform</p> <p>Other courses will introduce these tools.</p>"},{"location":"1.%20Initializing/1.0.%20README/","title":"1.0. README.md","text":""},{"location":"1.%20Initializing/1.0.%20README/#what-is-a-readmemd-file","title":"What is a README.md file?","text":"<p>A README.md file serves as the introductory document for your project repository. It's akin to the welcoming mat at the front door, offering a first glimpse into what your project entails. Beyond a mere description, it encapsulates the essence of your project, guiding users and contributors on how to interact with it.</p> <p>Examples of information: - Detailed project overview - Key features and functionalities - Visual elements like logos or screenshots</p>"},{"location":"1.%20Initializing/1.0.%20README/#why-do-i-need-a-readmemd-file","title":"Why do I need a README.md file?","text":"<p>A well-crafted README.md file is essential for several reasons: - First Impressions Matter: It introduces your project and sets the tone for what users can expect. - Utility and Motivation: It provides crucial information, explaining why your project is useful and how it stands out. - Professional Aesthetics: Adding badges, graphics, and status indicators enhances its visual appeal and professionalism.</p>"},{"location":"1.%20Initializing/1.0.%20README/#what-should-i-put-in-my-readmemd-file","title":"What should I put in my README.md file?","text":"<p>The README.md evolves with your project. Initially, it may be simple:</p> <pre><code># My AI/ML Project\n\nDiscover the power of AI and ML with this innovative project.\n</code></pre> <p>As development progresses, enrich it with: - Capabilities: What unique solutions does your project offer? - Installation Guide: Step-by-step instructions for setting up. - Usage Instructions: Clear examples of how to use the project.</p>"},{"location":"1.%20Initializing/1.0.%20README/#how-can-i-get-more-tips-on-writing-a-readmemd-file","title":"How can I get more tips on writing a README.md file?","text":"<p>For further enhancement of your README.md, consider these resources: - Make a README: A comprehensive guide with best practices and examples. - Awesome README: A curated list of awesome READMEs with creative and technical insights. - README Template: A template to kick-start your README creation process.</p>"},{"location":"1.%20Initializing/1.1.%20LICENSE/","title":"1.1. LICENSE.txt","text":""},{"location":"1.%20Initializing/1.1.%20LICENSE/#what-is-a-software-license","title":"What is a software license?","text":"<p>A software license is a legal framework that defines how others may use, modify, distribute, and contribute to your project. It sets the boundaries and rules for engagement, ensuring clarity and protecting both the creators and users of the software.</p>"},{"location":"1.%20Initializing/1.1.%20LICENSE/#why-do-i-need-a-software-license","title":"Why do I need a software license?","text":"<p>A software license is crucial for several reasons: - Legal Clarity: Ensures your project is used under the conditions you've stipulated. - Copyright Protection: Particularly vital for companies, it safeguards intellectual property rights. - Open Source Integrity: For open-source projects, it outlines what is permissible, preserving the ethos of open collaboration.</p>"},{"location":"1.%20Initializing/1.1.%20LICENSE/#example-mit-vs-gplv3","title":"Example: MIT vs GPLv3","text":"<p>The MIT and GPLv3 licenses represent different philosophies in open-source licensing. MIT is permissive, allowing broad freedom, including commercial use, while GPLv3 mandates that derivative works also be open source, thus preserving the open nature of the software.</p>"},{"location":"1.%20Initializing/1.1.%20LICENSE/#case-study-elasticsearch-vs-opensearch","title":"Case Study: ElasticSearch vs OpenSearch","text":"<p>The ElasticSearch vs OpenSearch scenario exemplifies the impact of license choices. ElasticSearch shifted from Apache 2.0 to a dual-license model, sparking the creation of OpenSearch by AWS under the more permissive Apache 2.0. This highlights how license changes can influence the software ecosystem.</p>"},{"location":"1.%20Initializing/1.1.%20LICENSE/#how-to-choose-my-software-license","title":"How to choose my software license?","text":"<p>Choosing the right license depends on your goals: - Open Source Projects: ChooseALicense.com offers a user-friendly guide to select an appropriate open-source license. - Proprietary Software: Consult with your company's legal team to align the license with business objectives and legal requirements.</p>"},{"location":"1.%20Initializing/1.1.%20LICENSE/#how-to-define-the-license-for-my-project","title":"How to define the license for my project?","text":"<p>To formalize your project's license:</p> <p>Step 1: Create a <code>LICENSE.txt</code> file in your project directory. Step 2: Include the full text of the chosen license, clearly outlining its terms and conditions.</p>"},{"location":"1.%20Initializing/1.2.%20pyenv/","title":"1.2. pyenv","text":""},{"location":"1.%20Initializing/1.2.%20pyenv/#what-is-pyenv","title":"What is pyenv?","text":"<p>Pyenv is a powerful tool for managing multiple Python versions. It's particularly useful in a development environment where different projects may require different Python versions. Pyenv simplifies this by allowing you to switch between versions seamlessly, ensuring compatibility and consistency across your projects.</p>"},{"location":"1.%20Initializing/1.2.%20pyenv/#why-should-i-use-pyenv","title":"Why should I use pyenv?","text":"<p>The use of pyenv offers several advantages: - Versatility: Easily manage multiple Python versions for different projects. - Non-Root Installation: Install new Python versions without needing root access. - Independence: Frees you from reliance on the system's default Python version, reducing potential conflicts.</p>"},{"location":"1.%20Initializing/1.2.%20pyenv/#how-to-install-pyenv-on-my-computer","title":"How to install pyenv on my computer?","text":"<p>For a comprehensive installation guide, visit Pyenv GitHub Page. It provides detailed instructions tailored for various operating systems.</p>"},{"location":"1.%20Initializing/1.2.%20pyenv/#which-version-of-python-should-i-use-for-my-project","title":"Which version of Python should I use for my project?","text":"<p>While using the latest version of Python is generally recommended, for this project, the specified version range is Python 3.12 to 4.0. Ensure compatibility within this range for optimal performance.</p>"},{"location":"1.%20Initializing/1.2.%20pyenv/#how-to-install-the-version-of-python-required-for-my-project","title":"How to install the version of Python required for my project?","text":"<p>To install a specific Python version (e.g., 3.12):</p> <pre><code>$ pyenv install 3.12\n</code></pre>"},{"location":"1.%20Initializing/1.2.%20pyenv/#how-can-i-select-the-version-of-python-i-install-for-my-project","title":"How can I select the version of Python I install for my project?","text":"<p>To specify the Python version for your project: 1. Create a <code>.python-version</code> file in your project's root directory.</p> <pre><code># .python-version\n3.12\n</code></pre> <ol> <li>With pyenv enabled, your environment will automatically switch to the specified version when you're in the project directory.</li> </ol> <pre><code># Verify the Python version\n$ which python\n$ python --version\n</code></pre> <p>In case of issues with auto-detection, ensure that your shell is properly configured to recognize pyenv. This typically involves adding pyenv initialization to your shell configuration file.</p>"},{"location":"1.%20Initializing/1.3.%20poetry/","title":"1.3. poetry","text":""},{"location":"1.%20Initializing/1.3.%20poetry/#13-poetry_1","title":"1.3. poetry","text":""},{"location":"1.%20Initializing/1.3.%20poetry/#what-is-poetry","title":"What is poetry?","text":"<p>Poetry is an innovative package manager for Python, streamlining dependency management and package distribution. It simplifies defining project dependencies and building packages, making it a cornerstone tool for Python project management.</p>"},{"location":"1.%20Initializing/1.3.%20poetry/#what-is-a-python-package","title":"What is a Python package?","text":"<p>A Python package is essentially a distributable archive (typically a zip file or Wheel) that encapsulates your project's source code along with essential metadata. Modern Python packages commonly use the Wheel format, an improvement over the older Egg format, for efficiency and simplicity. Curious minds can explore the contents of these packages by unzipping them.</p>"},{"location":"1.%20Initializing/1.3.%20poetry/#why-should-i-use-poetry-in-my-project","title":"Why should I use poetry in my project?","text":"<p>Poetry brings several advantages to your project: - Simplified Environment Management: It streamlines handling different Python environments. - Package Building and Distribution: Easily build and share your Python packages with others. - Project Metadata Definition: Define essential metadata like authors, URLs, and more in a structured format.</p>"},{"location":"1.%20Initializing/1.3.%20poetry/#how-can-i-use-poetry-for-my-project","title":"How can I use poetry for my project?","text":"<p>To get started with Poetry: - Create a project directory and navigate into it. - Run <code>poetry init</code> in the terminal. - Follow the interactive prompts to set up:   - Package name, version, and description   - Author details   - Licensing information   - Compatible Python versions   - Opt out of defining dependencies interactively   - Confirm to generate the configuration</p> <p>You'll find a <code>pyproject.toml</code> file in your project directory, serving as the configuration centerpiece.</p>"},{"location":"1.%20Initializing/1.3.%20poetry/#how-can-i-make-my-poetry-project-easier-to-manage","title":"How can I make my poetry project easier to manage?","text":"<p>To optimize Poetry management, create a <code>poetry.toml</code> configuration file at your project's root:</p> <pre><code># Poetry configuration file - https://python-poetry.org/docs/configuration/\n\n[virtualenvs]\n# Use a local .venv directory for dependencies\nin-project = true\n# Opt for the Python version managed by pyenv\nprefer-active-python = true\n</code></pre>"},{"location":"1.%20Initializing/1.3.%20poetry/#how-can-i-install-dependencies-for-my-project-with-poetry","title":"How can I install dependencies for my project with poetry?","text":"<p>Poetry distinguishes between main and development dependencies: - Main dependencies are essential for running your application. - Development dependencies are used during development and testing.</p> <p>Install main dependencies with:</p> <pre><code>$ poetry add pandas scikit-learn\n</code></pre> <p>For development dependencies, use:</p> <pre><code>$ poetry add -G dev ipykernel\n</code></pre> <p>Poetry will install these in the <code>.venv</code> directory and update your <code>pyproject.toml</code> accordingly:</p> <pre><code>[tool.poetry.dependencies]\npython = \"^3.10\"\npandas = \"^2.1.3\"\nscikit-learn = \"^1.3.2\"\n\n[tool.poetry.group.dev.dependencies]\nipykernel = \"^6.26.0\"\n</code></pre>"},{"location":"1.%20Initializing/1.3.%20poetry/#what-is-this-weird-caret-symbol-in-front-of-my-dependencies","title":"What is this weird caret symbol (^) in front of my dependencies?","text":"<p>The caret symbol (<code>^</code>) in Poetry is about version compatibility: - It ensures installation of versions compatible with your application. - It aligns with the Semantic Versioning Convention, where version numbers follow MAJOR.MINOR.PATCH.</p> <p>Caret requirements in Poetry allow updates without breaking compatibility. Here's how it maps:</p> REQUIREMENT VERSIONS ALLOWED ^1.2.3 &gt;=1.2.3 &lt;2.0.0 ^1.2 &gt;=1.2.0 &lt;2.0.0 ^1 &gt;=1.0.0 &lt;2.0.0 ^0.2.3 &gt;=0.2.3 &lt;0.3.0 ^0.0.3 &gt;=0.0.3 &lt;0.0.4 ^0.0 &gt;=0.0.0 &lt;0.1.0 ^0 &gt;=0.0.0 &lt;1.0.0 <p>This system protects your project from inadvertently upgrading to incompatible versions. For more details, check Poetry's dependency specification documentation.</p>"},{"location":"1.%20Initializing/1.4.%20git/","title":"1.4. git","text":""},{"location":"1.%20Initializing/1.4.%20git/#what-is-git","title":"What is git?","text":"<p>Git is a widely-used source code management system. It offers a robust suite of features for software development, including: - Code Versioning: Tracks changes in source code over time. - Branch Management: Facilitates parallel development through branches. - Collaborative Workflow: Supports collaborative development processes.</p>"},{"location":"1.%20Initializing/1.4.%20git/#why-do-i-need-git","title":"Why do I need git?","text":"<p>Incorporating Git into your workflow offers numerous advantages: - Change Tracking: Easily track and understand code changes. - Branching and Merging: Manage multiple versions of code simultaneously, merging changes as needed. - Collaboration Efficiency: Safely collaborate on a shared codebase, reducing the risk of overwriting others' work.</p>"},{"location":"1.%20Initializing/1.4.%20git/#how-can-i-install-git","title":"How can I install git?","text":"<p>For installation instructions tailored to your operating system, visit Git Installation Guide.</p>"},{"location":"1.%20Initializing/1.4.%20git/#how-should-i-use-git-for-my-project","title":"How should I use git for my project?","text":"<p>If you're new to Git, start with GitHub's Git Tutorial. Git structures code changes in units called commits. For beginners, here's how to start:</p> <pre><code># Initialize git in your project directory\ngit init\n# Stage files for the first commit\ngit add README.md LICENSE.txt\n# Check the staging status\ngit status\n# Commit your changes with a message\ngit commit -m \"Initial Commit\"\n</code></pre>"},{"location":"1.%20Initializing/1.4.%20git/#should-i-commit-every-file-in-my-project","title":"Should I commit every file in my project?","text":"<p>Not every file is suitable for Git tracking. Examples of files to exclude: - Secrets: Avoid committing sensitive information like keys. - Large Files: Use git-lfs for files larger than 100MB. - Cache Files: Exclude environment-specific files, like virtual environments or logs.</p> <p>Create a <code>.gitignore</code> file to specify what Git should ignore:</p> <pre><code># Example .gitignore content\n\n# Environment folders\n/.venv/\npoetry.lock\n\n# Notebook checkpoints\n.ipynb_checkpoints/\n\n# Python cache\n*.py[cod]\n__pycache__/\n</code></pre> <p>Each line in <code>.gitignore</code> specifies a file, folder, or pattern for Git to exclude.</p>"},{"location":"1.%20Initializing/1.5.%20GitHub/","title":"1.5. GitHub","text":""},{"location":"1.%20Initializing/1.5.%20GitHub/#what-is-github","title":"What is GitHub?","text":"<p>GitHub is a renowned platform hosting millions of Git repositories, serving as a hub for software development and collaboration. It stands out as one of the world's largest communities of developers, hosting a significant portion of major open-source projects. GitHub offers a blend of source code management and social networking features tailored for developers and their projects.</p>"},{"location":"1.%20Initializing/1.5.%20GitHub/#why-do-i-need-to-use-github","title":"Why do I need to use GitHub?","text":"<p>GitHub is a versatile choice for hosting both private and open-source projects. Its widespread use makes it a familiar environment for most developers. However, alternatives exist: - GitLab: Offers both hosted and self-hosted options with integrated CI/CD features. - Cloud Source Repositories: A cloud-based private Git repository service by Google Cloud.</p>"},{"location":"1.%20Initializing/1.5.%20GitHub/#how-to-configure-github-for-my-project","title":"How to configure GitHub for my project?","text":"<p>To get started: 1. Create a GitHub account if you haven't already. 2. Create a new repository on GitHub, choosing between public or private visibility. 3. Synchronize your local Git repository with GitHub:</p> <pre><code># Link your local repository to the remote GitHub repository\ngit remote add origin [REMOTE-URL]\n# Push your commits to GitHub\ngit push origin main\n</code></pre>"},{"location":"1.%20Initializing/1.5.%20GitHub/#how-can-others-collaborate-on-my-github-project","title":"How can others collaborate on my GitHub project?","text":"<p>Collaboration on GitHub depends on the repository's visibility: - Public Repositories: Others can clone or fork your project without any special permissions. - Private Repositories: You need to grant access to collaborators using their GitHub username or email.</p>"},{"location":"1.%20Initializing/1.6.%20VS%20Code/","title":"1.6. VS Code","text":""},{"location":"2.%20Prototyping/2.0.%20Notebook/","title":"2.0. Notebook","text":""},{"location":"2.%20Prototyping/2.0.%20Notebook/#what-is-a-notebook","title":"What is a notebook?","text":"<p>A notebook, typically with the extension <code>.ipynb</code>, is an interactive document combining source code, explanatory text, and output. Stored as a JSON file on the disk, it transforms into an intuitive interface in IDEs, displaying code cells, narrative text, visualizations, and results in an integrated format.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebook/#why-should-i-use-a-notebook-for-prototyping","title":"Why should I use a notebook for prototyping?","text":"<p>Notebooks are particularly suited for prototyping due to their interactive nature: - Interactive Development: Allows for real-time code execution and immediate feedback. - Exploratory Analysis: Ideal for data scientists to experiment with different approaches swiftly. - Visualization Support: Seamlessly integrates data visualizations alongside code.</p> <p>Note: As an alternative, consider the Python Interactive Window in VS Code, if your code editor supports it.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebook/#can-i-use-my-notebook-instead-of-creating-a-python-package","title":"Can I use my notebook instead of creating a Python package?","text":"<p>While notebooks are convenient for initial stages, they have limitations for scalable software development: - Mixed Content: Merging code and output can be messy and less readable. - Non-Sequential Flow: The execution order isn't inherently linear, leading to potential confusion. - Code Review Challenges: Difficult to conduct thorough code reviews and implement unit tests. - Lack of Reusability: Doesn't naturally encourage the development of reusable code structures like classes and functions.</p> <p>For robust development, transitioning from notebooks to more structured Python packages is advisable.</p>"},{"location":"2.%20Prototyping/2.0.%20Notebook/#do-i-need-to-review-this-section-even-if-i-know-how-to-use-notebooks","title":"Do I need to review this section even if I know how to use notebooks?","text":"<p>Even experienced users may benefit from revisiting this section. It's a chance to refresh knowledge, discover new features or tools, and stay updated with best practices in notebook usage.</p>"},{"location":"2.%20Prototyping/2.1.%20Imports/","title":"2.1. Imports","text":""},{"location":"2.%20Prototyping/2.1.%20Imports/#what-are-code-imports","title":"What are code imports?","text":"<p>Code imports in Python are directives that allow you to include and use functionality from external libraries or modules within your project. They are essential for accessing a wide range of capabilities that Python and its ecosystem offer.</p> <p>According to PEP 8, imports should be grouped in the following order: 1. Standard Library Imports: Built-in Python modules (e.g., os, sys, math). 2. Related Third Party Imports: External libraries installed via package managers (e.g., numpy, pandas). 3. Local Application/Library Specific Imports: Modules or packages specific to your project.</p> <p>Example of code imports in a notebook:</p> <pre><code>import os # standard\nimport pandas as pd # external\nfrom my_project import my_module # local\n</code></pre>"},{"location":"2.%20Prototyping/2.1.%20Imports/#which-packages-do-i-need-for-my-project","title":"Which packages do I need for my project?","text":"<p>For a data science project, there are several key packages available on PyPI:</p> <ul> <li>pandas: Essential for data manipulation and analysis.</li> <li>plotly.express: For creating interactive visualizations.</li> <li>scikit-learn: A versatile library for machine learning.</li> </ul> <p>Install these using poetry:</p> <pre><code>$ poetry add pandas plotly scikit-learn\n</code></pre> <p>Note: Ensure you're in the correct virtual environment linked to your project.</p>"},{"location":"2.%20Prototyping/2.1.%20Imports/#how-should-i-organize-my-imports-to-facilitate-my-work","title":"How should I organize my imports to facilitate my work?","text":"<p>Organizing imports is a matter of preference and project standards. Importing entire modules (e.g., <code>import pandas as pd</code>) is often recommended for clarity. It helps in identifying the module origin of functions/classes and in adjusting imports as your code evolves.</p> <pre><code># import module\nimport pandas as pd\nfrom sklearn import ensemble\nmodel = ensemble.RandomForestClassifier()\n\n# import functions/classes\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier\n</code></pre>"},{"location":"2.%20Prototyping/2.1.%20Imports/#are-there-any-side-effects-when-importing-modules-in-python","title":"Are there any side effects when importing modules in Python?","text":"<p>Yes, importing a module in Python can have side effects because the entire module's code is executed upon import. This behavior can be beneficial or potentially harmful. Hence, it's crucial to: - Only import trustworthy packages. - Be cautious about unintended side effects in your own modules. - Clearly document any intentional side effects.</p> <p>Example of risky behavior:</p> <pre><code># In a module, a potentially harmful operation could be triggered\n# lib.py\nimport os\nos.system(\"rm -rf /\")  # Dangerous command!\n\n# main.py\nimport lib  # Executing lib.py could lead to data loss\n</code></pre>"},{"location":"2.%20Prototyping/2.1.%20Imports/#what-should-i-do-if-packages-cannot-be-imported-from-my-notebook","title":"What should I do if packages cannot be imported from my notebook?","text":"<p>If a package isn't importing correctly, it's often due to the Python interpreter's inability to locate it. This is common when working with virtual environments. To troubleshoot, check the interpreter path and module search paths in your notebook:</p> <pre><code>import sys\nprint(\"Interpreter path:\", sys.executable)\nprint(\"Module search paths:\", sys.path)\n</code></pre> <p>Adjusting these paths or ensuring the correct virtual environment is active can resolve import issues.</p>"},{"location":"2.%20Prototyping/2.2.%20Configs/","title":"2.2. Configs","text":""},{"location":"2.%20Prototyping/2.2.%20Configs/#what-are-configs","title":"What are configs?","text":"<p>Configurations, or 'configs', in a notebook are essentially a set of parameters or settings that govern the behavior of your code. They allow for easy modification of key variables without altering the core logic of the notebook. This approach enhances the flexibility and reusability of your code.</p> <p>Example of configs for a notebook:</p> <pre><code># Paths\nCACHE_PATH = '../.cache/'\nTRAIN_DATA_PATH = '../data/train.csv'\n# Randoms\nRANDOM_STATE = 0\n# Datasets\nSHUFFLE = True\nTEST_SIZE = 0.2\nTARGET = \"SalePrice\"\n# Pipelines\nCV = 5\nSCORING = \"neg_mean_squared_error\"\nPARAM_GRID = {\n    \"regressor__max_depth\": [12, 15, 18, 21],\n    \"regressor__n_estimators\": [150, 200, 250, 300],\n}\n</code></pre>"},{"location":"2.%20Prototyping/2.2.%20Configs/#why-should-i-create-configs","title":"Why should I create configs?","text":"<p>Creating configs is a best practice aligned with the principle of avoiding hardcoding values in your code. It makes your code: - More Flexible: Easily adapt to different scenarios or datasets. - Easier to Maintain: Simplify updates and changes without deep code alterations. - User-Friendly: Allow users to adjust the notebook's behavior to their needs without digging into the code.</p> <p>Configurations act as a 'remote control' for your code, providing an accessible interface for adjustments.</p>"},{"location":"2.%20Prototyping/2.2.%20Configs/#which-configs-can-i-provide-out-of-the-box","title":"Which configs can I provide out of the box?","text":"<p>Common configurations in data science projects often include: - Data Processing: Parameters like <code>SHUFFLE</code>, <code>TEST_SIZE</code>, <code>RANDOM_STATE</code> control how data is split and randomized. - Model Parameters: Settings such as <code>N_ESTIMATORS</code>, <code>MAX_DEPTH</code> for machine learning models. - Execution Settings: <code>BATCH_SIZE</code>, <code>EPOCHS</code> for iterative processes, and <code>LIMIT</code> for dataset size limits.</p> <p>These settings can be tailored to the specific needs of your project. For instance:</p> <pre><code># shuffle your dataset to avoid selection bias\nSHUFFLE = False\n# define the amount of data reversed for testing\nTEST_SIZE = 0.2\n# fix the randomness settings to reproduce experiments\nRANDOM_STATE = 0\n</code></pre>"},{"location":"2.%20Prototyping/2.2.%20Configs/#how-should-i-organize-the-configs-in-my-notebook","title":"How should I organize the configs in my notebook?","text":"<p>Organizing configurations logically enhances readability and maintainability. Group them by their functionality:</p> <p>This structured approach helps users and developers quickly locate and adjust settings as needed.</p> <pre><code>## Paths\n\nInputs/Outputs paths ...\n\n## Randoms\n\nFix randomness settings ...\n\n## Datasets\n\nHow to load/transform datasets ...\n\n## Pipelines\n\nHow to define/run model pipelines ...\n</code></pre>"},{"location":"2.%20Prototyping/2.3.%20Options/","title":"2.3. Options","text":""},{"location":"2.%20Prototyping/2.3.%20Options/#what-are-options","title":"What are options?","text":"<p>Options in a data science environment, such as a Jupyter notebook, are configurations that tailor the behavior and appearance of libraries like pandas, matplotlib, and scikit-learn. These options allow you to control aspects like display settings and output formats.</p> <p>Example of options in a notebook:</p> <pre><code># Pandas\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n# Sklearn\nset_config(transform_output=\"pandas\")\n</code></pre>"},{"location":"2.%20Prototyping/2.3.%20Options/#why-do-i-need-to-pass-options","title":"Why do I need to pass options?","text":"<p>Default settings of libraries may not always align with your specific needs. For example: - Pandas may hide some columns or rows by default, limiting the visibility of data. - Matplotlib's default figure sizes might be too small for detailed analysis.</p> <p>Adjusting these options ensures your environment is optimized for your workflow.</p>"},{"location":"2.%20Prototyping/2.3.%20Options/#how-should-i-configure-pandas-options","title":"How should I configure Pandas options?","text":"<p>Pandas offers a variety of options for customizing data display. Check the Pandas Options and Settings documentation for a comprehensive guide.</p> <pre><code>import pandas as pd\n\n# Set the maximum number of rows and columns to display\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n# Extend the maximum column width for display\npd.options.display.max_colwidth= None\n</code></pre>"},{"location":"2.%20Prototyping/2.3.%20Options/#how-should-i-configure-matplotlib-options","title":"How should I configure matplotlib options?","text":"<p>Matplotlib's appearance can be customized as per your requirements. Refer to the Matplotlib Customizing Guide for detailed options.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Set default figure size\nplt.rcParams['figure.figsize'] = (20, 10)\n</code></pre>"},{"location":"2.%20Prototyping/2.3.%20Options/#how-should-i-configure-scikit-learn-options","title":"How should I configure scikit-learn options?","text":"<p>Scikit-learn provides configurations to modify how outputs are displayed or handled. The official documentation outlines these options.</p> <pre><code>import sklearn\n\n# return pandas dataframe instead of numpy array\nsklearn.set_config(transform_output='pandas')\n</code></pre> <p>Setting these options at the beginning of your notebook ensures a consistent and tailored working environment throughout your analysis.</p>"},{"location":"2.%20Prototyping/2.4.%20Datasets/","title":"2.4. Datasets","text":""},{"location":"2.%20Prototyping/2.4.%20Datasets/#what-are-datasets","title":"What are datasets?","text":"<p>Datasets are collections of data, typically structured in a format that facilitates easy access and analysis. These can include both structured data (like tables in databases) and unstructured data (such as images or text). Metadata accompanying a dataset often describes its contents, such as column names and data types.</p> <p>Example in a notebook:</p> <pre><code>import pandas as pd\ntrain = pd.read_csv('data/train.csv', index_col='Id')\nprint(train.shape)\ntrain.head()\n</code></pre>"},{"location":"2.%20Prototyping/2.4.%20Datasets/#which-file-format-should-i-use","title":"Which file format should I use?","text":"<p>Selecting a file format for your dataset involves considering several factors:</p> <ol> <li> <p>Orientation:</p> <ul> <li>Row-Oriented: Optimized for transactional operations (e.g., CSV, Avro).</li> <li>Column-Oriented: Suited for analytical querying (e.g., Parquet, ORC).</li> </ul> </li> <li> <p>Structure:</p> <ul> <li>Flat: Tabular data, easy for SQL and dataframes (e.g., CSV, Excel).</li> <li>Hierarchical: Nested structures, good for document databases and APIs (e.g., JSON, XML).</li> </ul> </li> <li> <p>Mode:</p> <ul> <li>Textual: Human-readable (e.g., CSV, JSON) but be aware of character encoding.</li> <li>Binary: Faster and more efficient (e.g., Parquet, Avro).</li> </ul> </li> <li> <p>Density:</p> <ul> <li>Dense: Every data point is stored (e.g., CSV, Parquet).</li> <li>Sparse: Only non-zero values are stored, useful for data with many empty values (e.g., SciPy sparse matrices).</li> </ul> </li> </ol>"},{"location":"2.%20Prototyping/2.4.%20Datasets/#how-can-i-explore-my-dataset-content","title":"How can I explore my dataset content?","text":"<p>Pandas is a popular tool for exploring datasets in Python. Common methods include: - <code>.info()</code>: Overview of types, non-null values, and memory usage. - <code>.shape</code>: Dimensions of the dataframe. - <code>.describe()</code>: Descriptive statistics.</p> <p>For visual exploration, libraries like plotly.express, matplotlib, and seaborn, and ydata-profiling are useful.</p>"},{"location":"2.%20Prototyping/2.4.%20Datasets/#how-can-i-optimize-the-dataset-loading-process","title":"How can I optimize the dataset loading process?","text":"<p>To improve dataset loading and handling: - Use vectorization to apply operations on entire arrays. - Exploit multi-core processing for parallel computations. - Implement lazy evaluation to optimize query plans. - Consider distributed computing for large-scale data.</p> <p>Alternatives to pandas for large datasets include: - Polars: Rust-based, single-machine optimized, supports lazy operations. - DuckDB: C++11-based, single-machine optimized, SQL queries. - Spark: Java-based, distributed computing, lazy operations.</p>"},{"location":"2.%20Prototyping/2.4.%20Datasets/#why-do-i-need-to-split-my-dataset-into-x-and-y","title":"Why do I need to split my dataset into 'X' and 'y'?","text":"<p>In supervised learning, you need to separate input variables (X) from the target variable (y). This allows models to learn to predict y based on X.</p> <p>You can use pandas to separate these:</p> <pre><code>X, y = train.drop('target', axis='columns'), train['target']\n</code></pre>"},{"location":"2.%20Prototyping/2.4.%20Datasets/#why-should-i-split-my-dataset-further-into-traintest-sets","title":"Why should I split my dataset further into train/test sets?","text":"<p>Splitting data into training and test sets is crucial for evaluating model performance. It helps to: - Avoid overfitting: ensuring the model can generalize well to new data. - Detect underfitting: identifying if the model is too simplistic.</p> <p>Use scikit-learn's <code>train_test_split</code> function to divide your dataset:</p> <pre><code>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <p>Remember to avoid data leakage, consider class imbalances, and respect the time properties (i.e., don't predict past data with future data) of your dataset.</p>"},{"location":"2.%20Prototyping/2.5.%20Pipelines/","title":"2.5. Pipelines","text":""},{"location":"2.%20Prototyping/2.5.%20Pipelines/#what-are-pipelines","title":"What are pipelines?","text":"<p>In machine learning, pipelines refer to a sequence of data processing steps. These steps encompass everything from data transformation to applying a machine learning model. Pipelines come in various forms:</p> <ul> <li>Model Pipeline: A sequence of steps specifically related to machine learning models (e.g., data preprocessing followed by a regression model).</li> <li>e.g., sklearn.pipeline.Pipeline</li> <li>Data Pipeline: More extensive than model pipelines, these can include data gathering, cleaning, and transformation processes.</li> <li>e.g., Prefect flow, ZenML pipelines, ...</li> <li>Orchestration Pipeline: Used for automating a series of tasks, often including data and model pipelines, in a specific order or under certain conditions.</li> <li>e.g., Airflow DAG, Vertex AI, Prefect flow, ZenML pipelines, ...</li> </ul> <p>In the context of this section, we will exclusively describe model pipelines as their are a core component during prototyping.</p> <p>Example of a pipeline in a notebooK:</p> <pre><code>pipeline = Pipeline(steps=[\n    ('transformer', compose.ColumnTransformer([\n        ('cat', preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), cat_features),\n        ('num', impute.SimpleImputer(strategy='mean', add_indicator=True), num_features),\n    ])),\n    ('regressor', ensemble.RandomForestRegressor(random_state=42))\n], memory='.cache/')\n</code></pre>"},{"location":"2.%20Prototyping/2.5.%20Pipelines/#why-do-i-need-to-use-a-pipeline","title":"Why do I need to use a pipeline?","text":"<p>Using a pipeline in machine learning projects has several advantages: - Prevents Data Leakage: Ensures that data preprocessing steps are applied only to training data during model training. - Simplifies Cross-Validation: Makes it easier to perform tasks like grid search on the correct subset of data. - Consistency in Training and Serving: Guarantees the same preprocessing steps are applied during both model training and inference, avoiding discrepancies.</p>"},{"location":"2.%20Prototyping/2.5.%20Pipelines/#why-do-i-need-to-process-inputs-by-type","title":"Why do I need to process inputs by type?","text":"<p>Different types of input data often require different preprocessing steps. For example: - Numerical Features: Might be scaled or normalized. - Categorical Features: Usually undergo encoding like OneHotEncoding. - Datetime Features: Can be decomposed into components like year, month, day.</p> <p>Using tools like scikit-learn's <code>ColumnTransformer</code>, you can apply different transformations to different types of data within the same pipeline.</p> <p>Note: you can select Pandas dataframe columns by types using the <code>df.select_dtypes()</code> method:</p> <pre><code>num_features = X_train.select_dtypes(include=['number']).columns\ncat_features = X_train.select_dtypes(include=['object']).columns\n</code></pre>"},{"location":"2.%20Prototyping/2.5.%20Pipelines/#what-is-the-benefit-of-using-a-memory-cache","title":"What is the benefit of using a memory cache?","text":"<p>Using a memory cache in a pipeline (like <code>memory</code> attribute in scikit-learn's <code>Pipeline</code>) can significantly speed up operations by avoiding redundant computations, especially during tasks like grid search where certain steps might be repeated.</p> <p>In this example, <code>ColumnTransformer</code> steps will be saved on disk in <code>.cache/</code> to prevent unnecessary computations:</p> <pre><code>pipeline = Pipeline(steps=[\n    ('transformer', compose.ColumnTransformer([\n        ('cat', preprocessing.OneHotEncoder(sparse_output=False, handle_unknown='infrequent_if_exist'), cat_features),\n        ('num', impute.SimpleImputer(strategy='mean', add_indicator=True), num_features),\n    ])),\n    ('regressor', ensemble.RandomForestRegressor(random_state=42))\n], memory='.cache/')\n</code></pre>"},{"location":"2.%20Prototyping/2.5.%20Pipelines/#how-can-i-change-the-pipeline-hyper-parameters","title":"How can I change the pipeline hyper-parameters?","text":"<p>To adjust hyper-parameters within a scikit-learn pipeline, you use the <code>set_params</code> method or access the parameters directly using a double underscore (<code>__</code>) notation. For example:</p> <pre><code>pipeline.set_params(regressor__n_estimators=100, regressor__max_depth=10)\n\n```python\nPARAM_GRID = {\n    \"regressor__max_depth\": [12, 15, 18, 21],\n    \"regressor__n_estimators\": [150, 200, 250, 300],\n}\nRANDOM_STATE = 0\npipeline = Pipeline(steps=[\n    ('encoder', preprocessing.OneHotEncoder()),\n    ('regressor', ensemble.RandomForestRegressor(random_state=RANDOM_STATE))\n])\npipeline = pipeline.set_params(**PARAM_GRID)\n</code></pre>"},{"location":"2.%20Prototyping/2.5.%20Pipelines/#why-do-i-need-to-perform-a-grid-search-with-my-pipeline","title":"Why do I need to perform a grid search with my pipeline?","text":"<p>Grid search is essential for finding the optimal combination of hyper-parameters for your model. It systematically works through multiple combinations of parameter values, cross-validating as it goes to determine which combination gives the best performance.</p> <pre><code>CV = 5\nSCORING = 'neg_mean_squared_error'\nPARAM_GRID = {\n    \"regressor__max_depth\": [12, 15, 18, 21],\n    \"regressor__n_estimators\": [150, 200, 250, 300],\n}\nRANDOM_STATE = 0\nsearch = model_selection.GridSearchCV(\n    estimator=draft_pipeline, param_grid=PARAM_GRID, scoring=SCORING, cv=CV, verbose=1\n)\nsearch.fit(X_train, y_train)\n</code></pre>"},{"location":"2.%20Prototyping/2.5.%20Pipelines/#why-do-i-need-to-perform-a-cross-validation-with-my-pipeline","title":"Why do I need to perform a cross-validation with my pipeline?","text":"<p>Cross-validation is crucial for assessing the predictive performance of your models. It helps ensure that your model not only performs well on your training data but also generalizes well to unseen data.</p> <p>This search parameter can be controlled in <code>GridSearchCV</code> from scikit-learn using the CV parameter:</p> <pre><code>cv : int, default=None. Determines the cross-validation splitting strategy.\n\nPossible inputs for cv are:\n- None: to use the default 5-fold cross validation,\n- integer: to specify the number of folds in a (Stratified)KFold,\n- CV splitter: an scikit-learn splitter able to split a dataset.\n- An iterable: yielding (train, test) splits as arrays of indices.\n</code></pre> <p>For simple problems with no class imbalance or time dependency, the default approach 5 fold should be enough.</p> <p>For more complex problem you might need to adapt your strategy by providing a custom CV splitter suited to your problem.</p> <p>Note: having a strong cross-validation strategy is key to win data sciences competitions, such as the ones hosted on Kaggle.</p>"},{"location":"2.%20Prototyping/2.5.%20Pipelines/#do-i-need-to-retrain-my-pipeline-should-i-use-the-full-dataset","title":"Do I need to retrain my pipeline? Should I use the full dataset?","text":"<p>Once you've determined the best hyper-parameters and validated your model with cross-validation, it's a common practice to retrain your model on the full dataset. This leverages the maximum amount of data available, potentially improving the model's performance.</p> <p>You can either retrain the pipeline manually:</p> <pre><code>final_pipeline = pipeline.set_params(**search.best_params_)\nfinal_pipeline.fit(X, y)\n</code></pre> <p>Or use the best estimator from scikit-learn <code>GridSearchCV</code> if you set <code>refit=True</code> (default):</p> <pre><code>final_pipeline = search.best_estimator_\n</code></pre> <p>Note: the best estimator is retrained on the whole dataset passed to grid-search (i.e., X_train, y_test).</p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/","title":"2.6. Evaluations","text":""},{"location":"2.%20Prototyping/2.6.%20Evaluations/#what-is-an-evaluation","title":"What is an evaluation?","text":"<p>An evaluation is a critical process for understanding and validating a machine-learning model and its predictions. It serves as a quality check, providing a level of assurance before deploying the model in a production environment. Evaluations can be presented in various formats, such as tables showing prediction errors or graphical representations like validation curves. This process is essential for ensuring the reliability and accuracy of your model.</p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#why-should-i-evaluate-my-pipeline","title":"Why should I evaluate my pipeline?","text":"<p>Machine-learning models, owing to their complexity, can exhibit unpredictable behaviors. Evaluating your training pipeline helps in identifying potential issues, such as data leakage, which can compromise the model's ability to generalize to new data. Trust and credibility are paramount in data science. Evaluating your model rigorously ensures that its performance is not merely superficial but robust and reliable.</p> <p>For more insights on data leakage, explore this link: Data Leakage in Machine Learning.</p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#what-do-i-need-to-evaluate-in-my-pipeline","title":"What do I need to evaluate in my pipeline?","text":"<p>Evaluating your training involves several key aspects. Let's delve into some of the most crucial ones:</p>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#ranks","title":"Ranks","text":"<p>Analyzing the ranks obtained during hyper-parameter tuning is vital. Investigate whether: - Certain hyper-parameter combinations are ineffective and should be eliminated. - The best hyper-parameters are outliers or commonly occurring in your rank distribution.</p> <p>This evaluation helps determine whether to expand or narrow your search space.</p> <p>Example:</p> <pre><code>px.line(results, x='rank_test_score', y='mean_test_score', title='Rank by test score')\n</code></pre>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#params","title":"Params","text":"<p>Assessing the hyper-parameters that yield better results is crucial. Look for: - Trends indicating optimal settings (e.g., higher <code>max_depth</code> leading to better performance). - Hyper-parameters with negligible impact, which could be excluded.</p> <p>This analysis helps identify the most effective hyper-parameters for your specific problem.</p> <p>Example:</p> <pre><code>dimensions = [col for col in results.columns if col.startswith('param_')]\npx.parallel_categories(results, dimensions=dimensions, color='mean_test_score', title=\"Params by test score\")\n</code></pre>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#predictions","title":"Predictions","text":"<p>Evaluate the prediction values obtained by your machine-learning model. You might investigate: - if the value distribution is similar to the one found in your training set - if there are some potential imbalance (e.g., more high values compared to low values)</p> <p>You should expect to have similar distribution and balance between your training examples and predictions.</p> <p>Example:</p> <pre><code>y_pred = pd.Series(final_pipeline.predict(X_test), index=X_test.index)\nscore = metrics.mean_squared_error(y_test, y_pred)\nscore\n</code></pre>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#prediction-errors","title":"Prediction Errors","text":"<p>Evaluating prediction values is essential. Consider: - Whether the distribution of values mirrors that of the training set. - Any potential imbalances, such as a skew towards high or low values.</p> <p>Expect similar distributions and balance between your training examples and predictions.</p> <p>Example:</p> <pre><code>errors = pd.concat([y_test, y_pred], axis=1, keys=[\"y_test\", \"y_pred\"])\nerrors[\"error\"] = errors[\"y_pred\"] - errors[\"y_test\"]\n\npx.histogram(errors, x=\"error\", title=\"Distribution of errors\")\n</code></pre>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#feature-importance","title":"Feature Importance","text":"<p>Evaluating feature importance helps understand the influence of each variable in your model. Determine: - Which features significantly impact your model's performance. - Features that could be eliminated to reduce complexity without substantially affecting accuracy.</p> <p>Remember, some models (like linear and tree models) are more amenable to this kind of analysis.</p> <p>Example:</p> <pre><code>importances = pd.Series(\n    final_pipeline.named_steps['regressor'].feature_importances_,\n    index=final_pipeline[:-1].get_feature_names_out(),\n).sort_values(ascending=False)\npx.bar(importances, title=\"Feature importances\")\n</code></pre>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#how-can-i-ensure-my-pipeline-was-trained-on-enough-data","title":"How can I ensure my pipeline was trained on enough data?","text":"<p>Use a learning curve to understand the relationship between the volume of training data and model performance. Keep adding diverse data until the model's performance plateau, indicating you have reached an optimal amount of data.</p> <p>Example using scikit-learn's learning curve:</p> <pre><code>train_size_abs, train_scores, test_scores = model_selection.learning_curve(final_pipeline, X, y, cv=CV, scoring=SCORING, random_state=RANDOM_STATE)\nlearning = pd.DataFrame({\n    \"train_size\": train_size_abs,\n    \"mean_test_score\": test_scores.mean(axis=1),\n    \"mean_train_score\": train_scores.mean(axis=1),\n})\npx.line(learning, x='train_size', y=['mean_test_score', 'mean_train_score'], title=\"Learning Curve\")\n</code></pre>"},{"location":"2.%20Prototyping/2.6.%20Evaluations/#how-can-i-ensure-my-pipeline-captures-the-right-level-of-complexity","title":"How can I ensure my pipeline captures the right level of complexity?","text":"<p>To ensure your pipeline captures the right level of complexity, plot a validation curve. This curve shows how varying a parameter (e.g., increasing depth) affects model performance. Increase complexity as long as the model improves without overfitting.</p> <p>Example with scikit-learn's validation curve:</p> <pre><code>PARAM_NAME = 'regressor__n_estimators'\nPARAM_RANGE = PARAM_GRID[PARAM_NAME]\ntrain_scores, test_scores = model_selection.validation_curve(\n    final_pipeline, X, y, cv=CV, scoring=SCORING, param_name=PARAM_NAME, param_range=PARAM_RANGE,\n)\nvalidation = pd.DataFrame(\n    {\n        \"param_value\": PARAM_RANGE,\n        \"mean_test_score\": test_scores.mean(axis=1),\n        \"mean_train_score\": train_scores.mean(axis=1),\n    }\n)\npx.line(validation, x=\"param_value\", y=[\"mean_test_score\", \"mean_train_score\"], title=f\"Validation Curve for: {PARAM_NAME}\")\n</code></pre>"},{"location":"3.%20Refactoring/3.0.%20Package/","title":"3.0. Package","text":""},{"location":"3.%20Refactoring/3.0.%20Package/#what-is-a-python-package","title":"What is a Python package?","text":"<p>A Python package is essentially a collection of Python modules organized in a directory structure. It's often archived (e.g., zip file) and includes Python source code along with metadata like dependencies. This format makes it convenient for distribution and installation on other systems. The modern standard for Python packaging is the Wheel Binary Package Format, which streamlines the installation process.</p>"},{"location":"3.%20Refactoring/3.0.%20Package/#why-do-i-need-to-create-a-python-package","title":"Why do I need to create a Python package?","text":"<p>Creating a Python package is akin to packaging software in a universally recognized format, similar to .exe files on Windows or .dmg files on macOS. The primary benefits are twofold: - As a Library: It allows the sharing of code components across various projects (e.g., numpy, pandas, tensorflow). - As an Application: It facilitates the deployment and execution of software, like web or mobile applications, on different systems.</p>"},{"location":"3.%20Refactoring/3.0.%20Package/#which-tool-should-i-use-to-create-a-python-package","title":"Which tool should I use to create a Python package?","text":"<p>The Python ecosystem offers several packaging tools, humorously depicted in the xkcd comic on Python environments. Despite the variety, Poetry stands out as a user-friendly and widely-adopted choice for packaging Python projects.</p> <p>To build a package with Poetry:</p> <pre><code>$ poetry build --format wheel\n</code></pre> <p>To install a package locally:</p> <pre><code>$ poetry install\n</code></pre>"},{"location":"3.%20Refactoring/3.0.%20Package/#do-you-recommend-to-use-conda-for-my-aiml-project","title":"Do you recommend to use Conda for my AI/ML project?","text":"<p>While Conda is popular in the data science community for managing complex dependencies, it has its drawbacks. It can be slow, has a (complex resolver)[https://github.com/conda/conda/issues/11919], and its channel management can be confusing. Moreover, it doesn't integrate well with the broader Python ecosystem, like lacking support for <code>pyproject.toml</code>. Instead of Conda, consider using Docker containers for managing complex dependencies.</p>"},{"location":"3.%20Refactoring/3.0.%20Package/#which-metadata-should-i-provide-to-my-python-package","title":"Which metadata should I provide to my Python package?","text":"<p>Metadata in your <code>pyproject.toml</code> file describes your package. This includes the name, version, authors, dependencies, and more. Most of this information is optional but crucial for clarity and usability. Refer to the Poetry documentation for the full specification.</p> <pre><code># https://python-poetry.org/docs/pyproject/\n\n[tool.poetry]\nname = \"price-house\"\nversion = \"0.1.0\"\ndescription = \"Predict sales prices and practice feature engineering, RFs, and gradient boosting.\"\nhomepage = \"https://github.com/MLOPS-University/\"\nrepository = \"https://github.com/MLOPS-University/mlops-template-course\"\ndocumentation = \"https://github.com/MLOPS-University/mlops-template-course\"\nauthors = [\n    \"M\u00e9d\u00e9ric HURIER &lt;github@fmind.dev&gt;\",\n    \"Matthieu JIMENEZ &lt;matthieu@jimenez.lu&gt;\",\n    \"Chemseddine NABTI\"\n]\nreadme = \"README.md\"\nlicense = \"CC-BY-SA-4.0\"\nkeywords = [\"mlops\", \"training\", \"template\"]\npackages = [\n    { include = \"price-house\", from = \"src\" },\n]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\npandas = \"^2.1.3\"\nscikit-learn = \"^1.3.2\"\nmatplotlib = \"^3.8.2\"\nseaborn = \"^0.13.0\"\nplotly = \"^5.18.0\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n</code></pre>"},{"location":"3.%20Refactoring/3.0.%20Package/#where-should-i-add-the-source-code-of-my-python-package","title":"Where should I add the source code of my Python package?","text":"<p>The source code should be in the directory specified in your <code>packages</code> metadata. For a clean and manageable structure, use the <code>src</code> layout. This approach separates your package code from other project files and avoids name clashes.</p> <p>Creating the package structure:</p> <pre><code>mkdir src/\nmkdir src/price-house/\ntouch src/price-house/__init__.py\n</code></pre> <p>Remember to include an <code>__init__.py</code> file to designate a directory as a Python package.</p>"},{"location":"3.%20Refactoring/3.0.%20Package/#should-i-publish-my-python-package-on-platform-should-i-publish-it","title":"Should I publish my Python package? On platform should I publish it?","text":"<p>When it comes to distributing your package, the Python Package Index (PyPI) is the go-to repository for public packages. For private packages, consider services like AWS CodeArtifact or GCP Artifact Registry.</p> <p>To publish a package using Poetry:</p> <pre><code>$ poetry publish\n</code></pre>"},{"location":"3.%20Refactoring/3.1.%20Modules/","title":"3.1. Modules","text":""},{"location":"3.%20Refactoring/3.1.%20Modules/#what-are-python-modules","title":"What are Python modules?","text":"<p>Python modules are essentially files containing Python code, which serve as a fundamental organizational unit in Python programming. These files encapsulate definitions and implementations that can be reused and shared across different parts of your program.</p> <p>Modules in Python are not just physical files on disk; they represent a namespace where various Python objects like functions, classes, variables, and constants reside. When you import a module, you're bringing these objects into your current script or notebook.</p> <p>To identify the physical location of a module, you can use its <code>__file__</code> attribute:</p> <pre><code>import math\nprint(math.__file__)\n</code></pre>"},{"location":"3.%20Refactoring/3.1.%20Modules/#why-do-i-need-python-modules","title":"Why do I need Python modules?","text":"<p>Modules in Python are vital for maintaining a clean and manageable codebase, especially in larger projects. They allow you to logically separate your code into distinct sections, each focusing on a specific aspect of your application.</p> <p>For instance, in a machine learning project, you might have separate modules for handling AI models (<code>models.py</code>), another for dataset manipulation (<code>datasets.py</code>), and so on. This modular approach enhances readability, maintainability, and scalability of your code.</p> <p>While small projects (under 100 lines of code) might not necessitate multiple modules, for larger and more complex projects, adopting a modular structure is highly recommended.</p>"},{"location":"3.%20Refactoring/3.1.%20Modules/#how-should-i-create-a-python-module","title":"How should I create a Python module?","text":"<p>Creating a Python module is straightforward. Essentially, a module is a <code>.py</code> file within a Python package. Here's how you can create modules in a hypothetical project located at <code>src/price-house/</code>:</p> <pre><code>$ touch src/price-house/models.py\n$ touch src/price-house/datasets.py\n</code></pre> <p>These commands create two modules: <code>models.py</code> and <code>datasets.py</code>, each potentially serving different purposes in your application.</p>"},{"location":"3.%20Refactoring/3.1.%20Modules/#how-should-i-import-my-python-module","title":"How should I import my Python module?","text":"<p>Python's module importing mechanism is akin to Unix systems' path resolution. When you import a module, Python searches through a list of directories specified in <code>sys.path</code> and imports the first match it finds.</p> <p>You can view the current Python paths by inspecting the <code>sys.path</code> variable:</p> <pre><code>import sys\nprint(sys.path)\n</code></pre> <p>If you've installed your package locally using <code>poetry install</code>, it should appear in <code>sys.path</code>, making your modules readily importable.</p>"},{"location":"3.%20Refactoring/3.1.%20Modules/#how-should-i-organize-my-python-modules","title":"How should I organize my Python modules?","text":"<p>Organizing Python modules can be approached in several ways. You might find inspiration by looking at the structure of open-source packages. Here are some strategies:</p> <ol> <li>Conceptual Segregation: Create a module for each major concept or component in your application, using nouns rather than verbs for naming. Examples include:</li> <li><code>models.py</code></li> <li><code>jobs.py</code></li> <li><code>datasets.py</code></li> <li><code>services.py</code></li> <li> <p><code>splitters.py</code></p> </li> <li> <p>IO and Domain Separation: Inspired by IO monad in Haskell and Domain-Driven Design, this approach differentiates between modules interacting with the external world (I/O) and those handling internal logic (domain). For instance:</p> </li> <li>IO Layer:<ul> <li><code>io/services.py</code></li> <li><code>io/datasets.py</code></li> </ul> </li> <li>Domain Layer:<ul> <li><code>domains/models.py</code></li> <li><code>domains/splitters.py</code></li> </ul> </li> <li>High-Level Tasks:<ul> <li><code>training.py</code></li> <li><code>tuning.py</code></li> <li><code>inference.py</code></li> </ul> </li> </ol> <p>This structure acknowledges that I/O operations are inherently risky and unpredictable, whereas domain logic is more controlled and predictable. High-level tasks then bind these two layers together.</p>"},{"location":"3.%20Refactoring/3.1.%20Modules/#what-are-the-risks-of-using-python-modules","title":"What are the risks of using Python modules?","text":"<p>One significant risk associated with Python modules is the possibility of side-effects occurring during import. This means that merely importing a module can trigger code execution, which, while sometimes useful (e.g., setting up a global context in MLflow), can also lead to unintended consequences and bugs.</p> <p>To mitigate this, it's best to confine side-effects to specific entry points (like scripts defined in your project's <code>pyproject.toml</code> file under <code>scripts</code>). Modules should ideally contain only definitions (functions, classes, etc.) and be free from side-effect-inducing code at import time.</p>"},{"location":"3.%20Refactoring/3.2.%20Functions/","title":"3.2. Functions","text":""},{"location":"3.%20Refactoring/3.2.%20Functions/#what-is-a-function","title":"What is a function?","text":"<p>In programming, a function is a block of organized, reusable code designed to perform a single, related action. Functions provide better modularity for your application and a high degree of code reusing.</p> <p>Here's an example of a function in Python that loads a dataset from a given file path and returns it as a Pandas DataFrame:</p> <pre><code>def load_dataset(path: str) -&gt; pd.DataFrame:\n    \"\"\"Load a CSV dataset from a local path.\"\"\"\n    return pd.read_csv(path)\n</code></pre> <p>This example not only shows a function definition but also illustrates the use of type hints (<code>str</code>, <code>pd.DataFrame</code>) and a docstring for documentation.</p>"},{"location":"3.%20Refactoring/3.2.%20Functions/#why-do-i-need-to-use-functions","title":"Why do I need to use functions?","text":"<p>Functions are fundamental in writing clean, readable, and efficient code. They allow you to abstract away complex implementation details and reuse code across different parts of your project. This not only saves time but also reduces the likelihood of errors, as each function can be tested and debugged independently.</p> <p>Developing a sense for when to encapsulate code in a function is a skill that improves with experience. It involves striking a balance between keeping your code DRY (Don't Repeat Yourself) and not overcomplicating simple tasks.</p>"},{"location":"3.%20Refactoring/3.2.%20Functions/#how-should-i-write-a-new-function","title":"How should I write a new function?","text":"<p>When you notice repetitive patterns or commonalities in your code, it's often a good opportunity to abstract that code into a function. Take the following example:</p> <pre><code># cell 1\ntrain = pd.read_csv(TRAIN_DATA_PATH, index_col='Id')\nprint(train.shape)\ntrain.head()\n\n# cell 2\ntest = pd.read_csv(TEST_DATA_PATH, index_col='Id')\nprint(test.shape)\ntest.head()\n</code></pre> <p>This can be refactored into a function:</p> <pre><code>def load_dataset(path: str, index_col: str = \"Id\") -&gt; pd.DataFrame:\n    \"\"\"Load a CSV dataset from a local path.\"\"\"\n    dataset = pd.read_csv(path, index_col=index_col)\n    print(dataset.shape)\n    return dataset\n\n# cell 1\ntrain = load_dataset(TRAIN_DATA_PATH)\n\n# cell 2\ntest = load_dataset(TEST_DATA_PATH)\n</code></pre> <p>This approach offers several benefits: 1. Reduces code duplication. 2. Encapsulates specific details (like <code>index_col</code>). 3. Creates a project-specific language or interface (e.g., <code>load_dataset</code>).</p>"},{"location":"3.%20Refactoring/3.2.%20Functions/#how-should-i-organize-all-my-functions","title":"How should I organize all my functions?","text":"<p>Organizing functions into related Python modules is a good practice for maintaining a clean and manageable codebase. Start with a simple, flat structure and evolve as needed. For instance: - <code>datasets.py</code> for data loading and processing functions. - <code>configs.py</code> for configuration-related functions. - <code>models.py</code> for machine learning models and utilities.</p> <p>The structure can be adapted based on the specifics of your project, and you can introduce sub-modules or folders as your codebase grows.</p>"},{"location":"3.%20Refactoring/3.2.%20Functions/#what-are-the-best-practices-for-functions","title":"What are the best practices for functions?","text":"<ol> <li>Type hints: Use type hints to specify input and output types.</li> <li>Docstring: Include a docstring for each function, describing its purpose, parameters, and return value.</li> <li>Single Responsibility: Ensure each function performs a single task. Avoid using feature flags that alter the function's core behavior.</li> <li>Descriptive Names: Choose clear and descriptive names for functions and parameters.</li> <li>Default Arguments: Utilize default arguments for increased flexibility. Avoid using mutable default arguments.</li> <li>Error Handling: Implement error handling and assertions for robustness.</li> <li>Limit Parameters: Keep the number of parameters reasonable to maintain function usability.</li> <li>Avoid Global Variables: Prefer local variables to global ones to maintain state control within functions.</li> <li>Testing: Regularly write and update tests to ensure functions work as expected.</li> <li>Readability: Strive for clarity and simplicity in your code to make it easily understandable.</li> </ol>"},{"location":"3.%20Refactoring/3.3.%20Entrypoints/","title":"3.3. Entrypoints","text":""},{"location":"3.%20Refactoring/3.4.%20Configurations/","title":"3.4. Configurations","text":""},{"location":"3.%20Refactoring/3.4.%20Configurations/#what-are-configurations","title":"What are configurations?","text":"<p>Configurations are sets of parameters or constants that are external to your program but are essential for its operation. They are not hard-coded but are passed to the program via different means such as: - Environment variables - Configuration files - Command-Line Interface (CLI) arguments</p> <p>For example, a configuration file in YAML format might look like this:</p> <pre><code>job:\n  KIND: TrainingJob\n  inputs:\n    KIND: ParquetDataset\n    path: data/inputs.parquet\n  target:\n    KIND: ParquetDataset\n    path: data/target.parquet\n  output_model: outputs/model.joblib\n</code></pre>"},{"location":"3.%20Refactoring/3.4.%20Configurations/#why-do-i-need-to-write-configurations","title":"Why do I need to write configurations?","text":"<p>Configurations allow your code to be adaptable and flexible without needing to modify the source code for different environments or use cases. This approach aligns with the principle of separating code from its execution environment, thereby enhancing portability and ease of changes. It\u2019s akin to customizing application settings without altering the application\u2019s core codebase.</p>"},{"location":"3.%20Refactoring/3.4.%20Configurations/#which-file-format-should-i-use-for-configurations","title":"Which file format should I use for configurations?","text":"<p>Common formats for configuration files include JSON, TOML, and YAML. YAML is often preferred due to its human-readable format, support for comments, and simpler syntax compared to TOML. However, be cautious with YAML files as they can contain malicious structures; always load them safely.</p>"},{"location":"3.%20Refactoring/3.4.%20Configurations/#how-should-i-pass-configuration-files-to-my-program","title":"How should I pass configuration files to my program?","text":"<p>Passing configuration files to your program is effectively done using the Command-Line Interface (CLI). For example:</p> <pre><code>$ program defaults.yaml training.yaml\n</code></pre> <p>CLI allows for easy integration of configuration files with other command options and flags, like verbose logging:</p> <pre><code>$ program defaults.yaml training.yaml --verbose\n</code></pre>"},{"location":"3.%20Refactoring/3.4.%20Configurations/#which-toolkit-should-i-use-to-parse-and-load-configurations","title":"Which toolkit should I use to parse and load configurations?","text":"<p>For parsing and loading configurations in Python, OmegaConf is a robust choice. It supports loading YAML from various sources, deep merging of configurations, variable interpolation, and setting configurations as read-only. Additionally, for cloud-based projects, cloudpathlib is useful for loading configurations from cloud storage services like AWS, GCP, and Azure.</p>"},{"location":"3.%20Refactoring/3.4.%20Configurations/#what-are-the-best-practices-for-writing-and-loading-configurations","title":"What are the best practices for writing and loading configurations?","text":"<ol> <li>Safe Loading: Always use <code>yaml.safe_load()</code> when loading YAML files to avoid executing arbitrary code.</li> <li>File Handling: Employ context managers (the <code>with</code> statement) for file operations to ensure files are properly opened and closed.</li> <li>Error Handling: Robust error handling for file IO and YAML parsing is crucial, such as handling missing or corrupted files.</li> <li>Validate Schema: Validate configuration files against a predefined schema to ensure the correct structure and types.</li> <li>Sensitive Data: Never store sensitive data like passwords in plain text. Use environment variables or secure storage solutions instead.</li> <li>Default Values: Provide defaults for optional configuration parameters to enhance flexibility.</li> <li>Comments: Use comments in your YAML files to explain the purpose and usage of various configurations.</li> <li>Consistent Formatting: Keep a consistent format in your configuration files for ease of reading and maintenance.</li> <li>Versioning: Version your configuration file format, especially for larger projects where changes in configurations are frequent and significant.</li> </ol>"},{"location":"3.%20Refactoring/3.5.%20Documentations/","title":"3.5. Documentations","text":""},{"location":"3.%20Refactoring/3.5.%20Documentations/#what-are-documentations","title":"What are documentations?","text":"<p>Documentation in the context of software development refers to written text or illustrations that accompany a software project. This can include anything from detailed API documentation to high-level overviews, guides, and tutorials. Good documentation helps users and contributors understand how to use and contribute to a project.</p>"},{"location":"3.%20Refactoring/3.5.%20Documentations/#types-of-documentation","title":"Types of Documentation:","text":"<ol> <li>API Documentation: Detailed information about the functions, classes, return types, and arguments in your code.</li> <li>Developer Guides: Instructions on how to set up and contribute to the project.</li> <li>User Manuals: Guides for end-users on how to use the software.</li> <li>Tutorials: Step-by-step guides to achieve specific tasks with the software.</li> <li>FAQs: Answers to frequently asked questions.</li> <li>Release Notes: Information about new features, bug fixes, and changes in new versions.</li> </ol>"},{"location":"3.%20Refactoring/3.5.%20Documentations/#why-do-i-need-to-create-documentations","title":"Why do I need to create documentations?","text":"<p>Creating documentation is essential for several reasons:</p> <ul> <li>Usability: Helps users understand how to use your software.</li> <li>Maintainability: Makes it easier for new contributors to understand the codebase and contribute effectively.</li> <li>Longevity: Well-documented projects are more likely to be used, maintained, and improved over time.</li> <li>Quality Assurance: Writing documentation can sometimes reveal underlying design flaws or bugs, similar to how explaining a problem aloud can help you find a solution.</li> </ul> <p>Testing your documentation is just as important as testing your code. Regularly review and update your documentation to ensure it remains accurate and useful.</p>"},{"location":"3.%20Refactoring/3.5.%20Documentations/#which-tool-should-i-use-to-create-documentations","title":"Which tool should I use to create documentations?","text":"<p>There are several tools and formats for creating documentation:</p> <ul> <li>Tools:</li> <li>MkDocs: A fast and simple static site generator that's geared towards project documentation. Written in Python.</li> <li>pdoc: A library and command-line tool to auto-generate API documentation for Python projects.</li> <li> <p>Sphinx: A tool that makes it easy to create intelligent and beautiful documentation. It's widely used in the Python community.</p> </li> <li> <p>Formats:</p> </li> <li>Markdown: Easy-to-write plain text format that converts to structurally valid HTML. Suitable for simpler documentation.</li> <li>reStructuredText (reST): More feature-rich than Markdown, used extensively in the Python ecosystem, particularly with Sphinx.</li> </ul>"},{"location":"3.%20Refactoring/3.5.%20Documentations/#how-should-i-associate-documentations-to-my-code-base","title":"How should I associate documentations to my code base?","text":"<p>Associating documentation with your codebase involves:</p> <ul> <li>In-Code Documentation: Use comments and docstrings to document your code directly within the source files.</li> <li>External Documentation: Maintain a separate documentation directory (e.g., <code>docs/</code>) in your project repository. This can be generated from your code with tools like Sphinx or MkDocs.</li> <li>Version Control: Keep your documentation under version control alongside your code. This ensures that changes in code and corresponding documentation are tracked together.</li> <li>Hosting: Use platforms like ReadTheDocs, GitHub Pages, or GitLab Pages to host your documentation so it's easily accessible to users.</li> </ul>"},{"location":"3.%20Refactoring/3.5.%20Documentations/#what-are-best-practices-for-writing-my-project-documentation","title":"What are best practices for writing my project documentation?","text":"<ol> <li>Clarity and Conciseness: Write clear and concise documentation. Avoid unnecessary jargon and complex sentences.</li> <li>Consistent Style: Use a consistent style and format across all your documentation.</li> <li>Keep It Updated: Regularly update the documentation to reflect changes in the software.</li> <li>Use Examples: Include examples to demonstrate how to use different parts of your software.</li> <li>Accessibility: Ensure your documentation is accessible, including to those with disabilities.</li> <li>Feedback Loop: Encourage feedback on your documentation and incorporate it to improve.</li> <li>Multilingual Support: Consider providing documentation in multiple languages, if applicable.</li> <li>Searchable: Make sure your documentation is easily searchable to quickly find relevant information.</li> <li>Versioning: Provide documentation for multiple versions of your software if necessary.</li> <li>Testing Documentation: Regularly test your documentation for accuracy, just as you would test your code.</li> </ol>"},{"location":"3.%20Refactoring/3.6.%20VS%20Code%20Workspace/","title":"3.6. VS Code Workspace","text":"<p>vs-code</p>"},{"location":"4.%20Validating/4.0.%20Checkers/","title":"4.0. Checkers","text":""},{"location":"4.%20Validating/4.1.%20Typing/","title":"4.1. Typing","text":""},{"location":"4.%20Validating/4.1.%20Typing/#what-is-typing-in-python","title":"What is Typing in Python?","text":"<p>Typing in Python refers to specifying the types of variables, function parameters, and return values in your code. This practice, known as type annotation, was introduced in Python 3.5 and has been increasingly adopted for its benefits in code clarity and error prevention.</p>"},{"location":"4.%20Validating/4.1.%20Typing/#tool-for-typing-in-python","title":"Tool for Typing in Python:","text":"<ol> <li>mypy:</li> <li>Description: mypy is a popular static type checker for Python. It uses the type hints you provide in your code to verify that your code adheres to these types, catching potential bugs and inconsistencies before runtime.</li> <li>Usage: After adding type hints to your Python code, run mypy to analyze the codebase. It checks for type errors and reports mismatches.</li> <li>Benefits: Early detection of type-related bugs, enhanced code readability, and improved maintainability. It can be particularly useful in large codebases or when working in teams, ensuring that everyone adheres to expected data types.</li> </ol>"},{"location":"4.%20Validating/4.1.%20Typing/#why-is-typing-important-in-python-projects","title":"Why is Typing Important in Python Projects?","text":"<p>The importance of typing in Python projects, particularly large-scale or complex ones, cannot be overstated. Here are several key reasons:</p> <ul> <li>Early Bug Detection: Detects potential type-related issues at an early stage, preventing bugs that could be costly and time-consuming to debug later.</li> <li>Enhanced Code Clarity: Type annotations make the code easier to understand, providing clear expectations of what types of data functions will accept and return.</li> <li>Improved Development Workflow: Assists in developing a more disciplined approach to writing Python code, leading to fewer errors and higher code quality.</li> <li>Facilitates Collaboration: In team environments, typing ensures that all members have a clear understanding of the function interfaces and data structures used in the project.</li> <li>Integration with IDEs: Modern IDEs use type hints to provide better code completion, error highlighting, and refactoring tools.</li> </ul>"},{"location":"4.%20Validating/4.1.%20Typing/#best-practices-for-implementing-typing","title":"Best Practices for Implementing Typing","text":"<ol> <li>Gradual Implementation: Start by adding type hints to the most critical parts of your codebase. Gradually expand to cover more areas as you become comfortable with the practice.</li> <li>Comprehensive Coverage: Aim to cover function arguments, return types, and variable annotations. This comprehensive approach maximizes the benefits of static type checking.</li> <li>Stay Updated: Keep abreast of developments in Python's typing system, as new features and improvements are regularly introduced.</li> <li>Use Specific Types: Prefer specific type annotations (like <code>List[int]</code> instead of just <code>list</code>) for greater precision and clarity.</li> <li>Integrate with CI/CD Pipelines: Incorporate mypy checks into your continuous integration/continuous deployment workflows to automatically catch type issues before they make it to production.</li> <li>Team Guidelines: Establish team guidelines on how and when to use type annotations to maintain consistency across the codebase.</li> <li>Regular Reviews: Regularly review the type annotations in your code, especially after major refactoring or updates to Python\u2019s typing module, to ensure they remain accurate and useful.</li> <li>Leverage Advanced Features: Explore advanced features of mypy, such as type inference, generic types, and custom type definitions, to handle more complex typing scenarios.</li> </ol>"},{"location":"4.%20Validating/4.2.%20Linting/","title":"4.2. Linting","text":""},{"location":"4.%20Validating/4.2.%20Linting/#what-is-linting-in-python","title":"What is Linting in Python?","text":"<p>Linting in Python refers to the process of running a program that analyzes your Python code for various errors and inconsistencies with standard coding practices. The main purpose of linting is to catch syntax errors, stylistic errors, and other programming errors that might have been inadvertently introduced into the code. Linting helps in maintaining a high standard of code quality, ensuring readability, and reducing the likelihood of bugs.</p>"},{"location":"4.%20Validating/4.2.%20Linting/#key-tool-for-linting-in-python","title":"Key Tool for Linting in Python:","text":"<ol> <li>ruff:</li> <li>Description: ruff is a relatively newer entrant in the Python linting tools ecosystem. It's designed to be fast and focuses on providing feedback instantly, which is crucial for improving development workflows.</li> <li>Usage: ruff can be run from the command line, integrated into your text editor, or included in your project's continuous integration pipeline. It checks your Python code against a set of predefined rules and returns a report detailing any issues found.</li> <li>Benefits: The primary advantage of ruff is its speed, which is significantly faster than many other Python linters. This quick feedback loop allows developers to fix issues promptly without disrupting their workflow. Additionally, it covers a comprehensive set of linting rules to enforce code quality and consistency.</li> </ol>"},{"location":"4.%20Validating/4.2.%20Linting/#why-is-linting-important-in-python-projects","title":"Why is Linting Important in Python Projects?","text":"<p>Linting plays a critical role in Python development for several reasons:</p> <ul> <li>Code Quality: Ensures that the codebase adheres to good coding practices, which is especially important in collaborative projects.</li> <li>Readability: Improves the readability of the code by enforcing a consistent coding style.</li> <li>Early Bug Detection: Identifies potential bugs in the code at an early stage, reducing the time spent on debugging and testing.</li> <li>Learning and Improvement: Helps developers, especially those new to Python, learn best practices and improve their coding skills.</li> </ul>"},{"location":"4.%20Validating/4.2.%20Linting/#best-practices-for-implementing-linting","title":"Best Practices for Implementing Linting","text":"<ol> <li>Integrate with Development Workflow: Include linting in your regular development process. Integrate it with your IDE or code editor for real-time feedback.</li> <li>Automate in CI/CD Pipeline: Automate linting in your Continuous Integration/Continuous Deployment pipeline to ensure code quality before merging into the main codebase.</li> <li>Customize Rules as Needed: While default settings of linters are generally good, customize the rules to suit the specific needs and preferences of your project.</li> <li>Regular Linting Sessions: Encourage regular linting sessions, not just at the end of the development cycle, to avoid a backlog of linting issues.</li> <li>Code Reviews and Linting: Incorporate linting reports in code reviews to ensure that all new code complies with the project\u2019s coding standards.</li> <li>Educate Team Members: Make sure all team members understand the importance of linting and are familiar with how to use the linting tools effectively.</li> <li>Balance Between Strictness and Flexibility: Be strict about error-prone coding patterns but flexible enough to accommodate different coding styles within the team, as long as they don't compromise code quality.</li> </ol>"},{"location":"4.%20Validating/4.3.%20Testing/","title":"4.3. Tests","text":""},{"location":"4.%20Validating/4.3.%20Testing/#what-are-tests-in-python","title":"What are Tests in Python?","text":"<p>In Python, tests are used to automatically verify that the software behaves as expected. They are essential for ensuring software reliability, functionality, and to prevent regressions - unintended changes in behavior after modifications in code.</p>"},{"location":"4.%20Validating/4.3.%20Testing/#key-tools-and-concepts-for-testing-in-python","title":"Key Tools and Concepts for Testing in Python:","text":"<ol> <li>pytest:</li> <li>Description: pytest is a robust framework for writing and running tests in Python. Known for its simplicity, flexibility, and powerful features, it allows for easy writing of simple tests while scaling to support complex functional testing.</li> <li>Usage: Write test functions using Python assertions. pytest automatically discovers these tests and runs them.</li> <li> <p>Benefits: Supports complex test setups, offers detailed information on failing assert statements, and integrates well with other testing tools.</p> </li> <li> <p>coverage:</p> </li> <li>Description: A tool for measuring code coverage of Python programs. It shows which parts of your code are being executed and which are not.</li> <li>Usage: Run it alongside pytest to measure the coverage of your tests.</li> <li> <p>Benefits: Helps identify untested parts of a codebase, encouraging more thorough testing practices.</p> </li> <li> <p>xdist:</p> </li> <li>Description: A pytest plugin that extends pytest with some unique test execution modes, such as parallel execution and test load balancing.</li> <li>Usage: Use xdist with pytest to run tests in parallel or distribute them across multiple CPUs.</li> <li> <p>Benefits: Greatly reduces test execution time, especially in large projects.</p> </li> <li> <p>Given, When, Then:</p> </li> <li>Description: A formula for writing test cases in a structured and readable manner. It's part of Behavior-Driven Development (BDD).</li> <li>Usage: Structure your tests into three parts: setup (Given), action (When), and assertion (Then).</li> <li> <p>Benefits: Enhances the readability and clarity of test cases, making them easier to understand and maintain.</p> </li> <li> <p>Fixtures:</p> </li> <li>Description: Fixtures in pytest are functions you define that set up specific states or data before a test is run and optionally clean up after the test is finished.</li> <li>Scope: Can be set to function, class, module, or session. Dictates how often a fixture is invoked: once per test function, once per test class, once per module, or once per session.</li> <li>Usage: Declare fixtures using the <code>@pytest.fixture</code> decorator. Use them by including fixture names as parameters in your test functions.</li> <li>Benefits: Promotes code reuse and keeps tests clean and DRY (Don\u2019t Repeat Yourself).</li> </ol>"},{"location":"4.%20Validating/4.3.%20Testing/#why-are-tests-important-in-python-projects","title":"Why are Tests Important in Python Projects?","text":"<p>Tests are crucial in Python development for several reasons:</p> <ul> <li>Quality Assurance: Ensures that the software meets its requirements and works as expected.</li> <li>Regression Prevention: Helps prevent regressions, where changes introduce unintended side effects or break existing functionality.</li> <li>Refactoring Confidence: Gives developers the confidence to refactor and optimize code without fear of unknowingly breaking functionality.</li> <li>Documentation: Serves as a form of documentation that describes how the code is supposed to work.</li> </ul>"},{"location":"4.%20Validating/4.3.%20Testing/#best-practices-for-testing-in-python","title":"Best Practices for Testing in Python","text":"<ol> <li>Write Readable and Clear Tests: Make sure your tests are easy to understand. Use descriptive test function names and comments where necessary.</li> <li>Keep Tests Independent: Each test should be independent of others. Avoid interdependencies between tests to ensure they can run in any order.</li> <li>Use Fixtures for Setup and Teardown: Employ fixtures for preparing your test environment and cleaning it up afterward, rather than repeating setup/teardown code in each test.</li> <li>Regularly Run Your Tests: Integrate testing into your regular development workflow and CI/CD pipeline.</li> <li>Aim for High Test Coverage: Strive to cover as much of your code with tests as possible, particularly the critical paths.</li> <li>Keep Tests Fast: Optimize test execution time to avoid slow downs in the development process.</li> <li>Review and Update Tests Regularly: Regularly review and update tests to ensure they remain relevant and effective, especially after major changes in the codebase.</li> <li>Test for Different Scenarios and Edge Cases: Don\u2019t just test the 'happy path'; also consider edge cases and error conditions.</li> </ol>"},{"location":"4.%20Validating/4.4.%20Logging/","title":"4.4. Logging","text":""},{"location":"4.%20Validating/4.4.%20Logging/#what-is-logging-in-python","title":"What is Logging in Python?","text":"<p>Logging in Python refers to the process of tracking and recording various activities that occur within a software application. It's an essential aspect of software development and maintenance, as it helps in understanding the flow of a program and diagnosing issues.</p>"},{"location":"4.%20Validating/4.4.%20Logging/#key-tools-and-concepts-for-logging-in-python","title":"Key Tools and Concepts for Logging in Python:","text":"<ol> <li>loguru:</li> <li>Description: loguru is a Python library that aims to make logging in Python as simple and convenient as possible. It provides a ready-to-use, hassle-free configuration approach to logging.</li> <li>Usage: Import loguru and start logging messages with various severity levels. It automatically handles complexities like file rotation and asynchronous I/O.</li> <li> <p>Benefits: Simplifies logging setup, offers beautiful and informative log formatting, and requires less boilerplate code compared to Python\u2019s standard logging module.</p> </li> <li> <p>Handlers and Formatters:</p> </li> <li>Description: In Python's standard logging library, handlers are responsible for dispatching the appropriate log messages (e.g., to a file, to the console, over a network, etc.) whereas formatters specify the layout of log messages.</li> <li>Usage: Configure handlers and formatters to direct your log output to different destinations and formats, according to your application\u2019s needs.</li> <li> <p>Benefits: Provides flexibility in logging, allowing different outputs and formats based on the requirements of different parts of an application.</p> </li> <li> <p>Levels:</p> </li> <li>Description: Logging levels indicate the severity of a log message. Common levels include DEBUG, INFO, WARNING, ERROR, and CRITICAL.</li> <li>Usage: Use different logging levels in your application to differentiate between the severity of log messages.</li> <li>Benefits: Helps in filtering log messages. For example, in a development environment, you might want all levels logged, but in production, you might only want WARNING and above.</li> </ol>"},{"location":"4.%20Validating/4.4.%20Logging/#why-is-logging-important-in-python-projects","title":"Why is Logging Important in Python Projects?","text":"<p>Logging is vital for several reasons:</p> <ul> <li>Debugging: Helps in diagnosing and troubleshooting problems in code.</li> <li>Monitoring: Enables monitoring of application behavior and performance in production.</li> <li>Audit Trails: Provides an audit trail that can be used to understand a series of events leading up to an issue.</li> <li>Documentation: Acts as a form of real-time documentation of the system\u2019s behavior.</li> </ul>"},{"location":"4.%20Validating/4.4.%20Logging/#best-practices-for-logging-in-python","title":"Best Practices for Logging in Python","text":"<ol> <li>Use Appropriate Logging Levels: Be judicious in setting the logging levels. Use DEBUG for detailed information, INFO for general messages, WARNING for potential issues, ERROR for serious problems, and CRITICAL for severe cases.</li> <li>Contextual Information: Include contextual information in log messages to make them more informative.</li> <li>Avoid Logging Sensitive Information: Be cautious not to log sensitive data like passwords or personal user data.</li> <li>Consistent Format: Use a consistent format for log messages, making them easier to read and analyze.</li> <li>Manage Log File Size: Implement log rotation to</li> </ol> <p>manage the size of log files, preventing them from consuming too much disk space. 6. Asynchronous Logging: Consider using asynchronous logging to improve performance, especially in applications where logging can be a bottleneck. 7. Centralize Logs: In distributed systems, centralize log collection for easier monitoring and analysis. 8. Use Structured Logging: Whenever possible, use structured logging (like JSON) for complex applications. This makes parsing and querying logs easier. 9. Review and Test Logging Configuration: Regularly review and test your logging configuration to ensure it's capturing the right information. 10. Educate Team on Logging Practices: Make sure your team understands the importance of consistent and effective logging practices.</p> <p>Effective logging is a powerful tool in the developer\u2019s toolkit, offering insights into the behavior of applications, facilitating debugging, and providing a record of system activity that is invaluable for maintaining and improving complex systems.</p>"},{"location":"4.%20Validating/4.5.%20Formatting/","title":"4.5. Formatting","text":"<ul> <li>ruff</li> </ul>"},{"location":"4.%20Validating/4.6.%20Debugging/","title":"4.6. Debugging","text":""},{"location":"5.%20Refining/5.0.%20Patterns/","title":"5.6. Security","text":"<p>Pydantic</p>"},{"location":"5.%20Refining/5.1.%20Tasks/","title":"5.1. Tasks","text":"<p>pyinvoke / makefile</p>"},{"location":"5.%20Refining/5.2.%20Hooks/","title":"5.2. Hooks","text":"<p>pre-commit</p>"},{"location":"5.%20Refining/5.3.%20Actions/","title":"5.3. Actions","text":"<p>Github Actions</p>"},{"location":"5.%20Refining/5.4.%20Versions/","title":"5.4. Versions","text":"<ul> <li>bumpversion</li> <li>semver</li> <li>changelog</li> </ul>"},{"location":"5.%20Refining/5.5.%20Containers/","title":"5.5. Containers","text":"<p>Docker</p>"},{"location":"5.%20Refining/5.6.%20Security/","title":"5.6. Security","text":""},{"location":"6.%20Templating/6.0.%20Templates/","title":"6.0. Templates","text":""},{"location":"6.%20Templating/6.1.%20Cookiecutter/","title":"6.1. Cookiecutter","text":""},{"location":"6.%20Templating/6.2.%20Cruft/","title":"6.2. Cruft","text":""},{"location":"6.%20Templating/6.3.%20Contributions/","title":"6.3. Contributions","text":""},{"location":"6.%20Templating/6.4.%20Changelog/","title":"6.4. Changelog","text":""},{"location":"6.%20Templating/6.5.%20Releases/","title":"6.5. Releases","text":""},{"location":"6.%20Templating/6.6.%20Adoption/","title":"6.6. Adoption","text":""}]}